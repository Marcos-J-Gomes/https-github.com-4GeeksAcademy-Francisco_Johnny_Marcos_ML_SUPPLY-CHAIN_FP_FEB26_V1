{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3053e060",
   "metadata": {},
   "source": [
    "# Step 6: Feature Engineering\n",
    "## 6.1 Outlier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07713fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Creating boxplot to understand the outliers in each variable\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m ncols = \u001b[32m3\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m nrows = \u001b[43mmath\u001b[49m.ceil(\u001b[38;5;28mlen\u001b[39m(df_final.columns.tolist()) / ncols)\n",
      "\u001b[32m      4\u001b[39m fig, axes = plt.subplots(nrows= nrows, ncols= ncols, figsize= (\u001b[32m15\u001b[39m, \u001b[32m4\u001b[39m * nrows))\n",
      "\u001b[32m      6\u001b[39m axes = axes.flatten()\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating boxplot to understand the outliers in each variable\n",
    "ncols = 3\n",
    "nrows = math.ceil(len(df_final.columns.tolist()) / ncols)\n",
    "fig, axes = plt.subplots(nrows= nrows, ncols= ncols, figsize= (15, 4 * nrows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(df_final.columns.tolist()):\n",
    "    sns.boxplot(data= df_final, y= col, ax= axes[i], color= 'salmon')\n",
    "for j in range(len(df_final.columns.tolist()), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647db8f",
   "metadata": {},
   "source": [
    "### 6.1.1 Conclusion Boxplots Charts\n",
    "- Those boxplots are the final \"green light\" we needed to proceed with the model. By visualizing the outliers so clearly, I’ve identified exactly where the KNN algorithm might get \"pulled\" in the wrong direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary coding to replace outliers creating upper and lower limits\n",
    "df_WITH_outliers_baseCol = df_col_base.copy()\n",
    "df_WITHOUT_outliers_baseCol = df_col_base.copy()\n",
    "\n",
    "df_WITH_outliers_optCol = df_col_opt.copy()\n",
    "df_WITHOUT_outliers_optCol = df_col_opt.copy()\n",
    "\n",
    "outliers_cols_base = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'] # The target variable price can't be modified so we leave as it is.\n",
    "outliers_cols_opt = ['fixed acidity', 'volatile acidity', 'citric acid', 'chlorides', 'total sulfur dioxide', 'sulphates', 'alcohol']\n",
    "\n",
    "def replace_outliers(column, data_df):\n",
    "   col_stats = data_df[column].describe()\n",
    "   col_iqr = col_stats[\"75%\"] - col_stats[\"25%\"]\n",
    "   upper_limit = round(float(col_stats[\"75%\"] + 1.5 * col_iqr), 2)\n",
    "   lower_limit = round(float(col_stats[\"25%\"] - 1.5 * col_iqr), 2)\n",
    "\n",
    "   if lower_limit < 0: lower_limit = min(df[column])\n",
    "   # Let's take out upper outliers \n",
    "   data_df[column] = data_df[column].apply(lambda x: x if (x <= upper_limit) else upper_limit)\n",
    "   # Let's take out lower outliers \n",
    "   data_df[column] = data_df[column].apply(lambda x: x if (x >= lower_limit) else lower_limit)\n",
    "   return data_df.copy(), [lower_limit, upper_limit]\n",
    "\n",
    "outliers_dict_base = {}\n",
    "for column in outliers_cols_base:\n",
    "   df_WITHOUT_outliers_baseCol, limits = replace_outliers(column, df_WITHOUT_outliers_baseCol)\n",
    "   outliers_dict_base.update({column: limits})\n",
    "\n",
    "outliers_dict_opt= {}\n",
    "for column in outliers_cols_opt:\n",
    "   df_WITHOUT_outliers_optCol, limits = replace_outliers(column, df_WITHOUT_outliers_optCol)\n",
    "   outliers_dict_opt.update({column: limits})\n",
    "\n",
    "print(f\"Limits using base columns: {outliers_dict_base}\") # This jason needs to be saved\n",
    "print(f\"Limits using optimized columns: {outliers_dict_opt}\") # This jason needs to be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the json dictionary for outliers limit\n",
    "with open('../data/interim/outliers_dict_base.json', 'w') as f:\n",
    "     json.dump(outliers_dict_base, f)\n",
    "\n",
    "with open('../data/interim/outliers_dict_opt.json', 'w') as f:\n",
    "     json.dump(outliers_dict_opt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5f5dc",
   "metadata": {},
   "source": [
    "## 6.2 Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The list below, will show if dataset WITH outliers has any null value for the variables:\")\n",
    "print(df_WITH_outliers_baseCol.isnull().sum().sort_values(ascending= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9428ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The list below, will show if dataset WITHOUT outliers has any null value for the variables:\")\n",
    "print(df_WITHOUT_outliers_baseCol.isnull().sum().sort_values(ascending= False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c90e14",
   "metadata": {},
   "source": [
    "## 6.3 Inference of New Features\n",
    "* Proposed Engineering for Both Sets\n",
    "    - The Acidity Index: Create a total_acidity column ($fixed + volatile + citric$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fcbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_engineering(df_input, is_base= True):\n",
    "    \n",
    "    # Applied to both sets\n",
    "    df_input['total_acidity'] = df_input['fixed acidity'] + df_input['volatile acidity'] + df_input['citric acid']\n",
    "    \n",
    "    return df_input\n",
    "\n",
    "# Generate the new experimental sets\n",
    "df_WITH_outliers_baseCol = apply_engineering(df_WITH_outliers_baseCol, is_base=True)\n",
    "df_WITHOUT_outliers_baseCol = apply_engineering(df_WITHOUT_outliers_baseCol, is_base=True)\n",
    "\n",
    "df_WITH_outliers_optCol = apply_engineering(df_WITH_outliers_optCol, is_base=False)\n",
    "df_WITHOUT_outliers_optCol = apply_engineering(df_WITHOUT_outliers_optCol, is_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating << NEW >> heatmap chart to analyze complete variables correlation\n",
    "cols_num = df_WITH_outliers_baseCol.select_dtypes(include= ['number']).columns.tolist()\n",
    "fig, ax = plt.subplots(figsize= (15, 9))\n",
    "sns.heatmap(df_WITH_outliers_baseCol[cols_num].corr(method= \"pearson\"), annot= True, fmt= \".2f\", cmap= \"coolwarm\", ax= ax).tick_params(axis= 'x', rotation= 45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a27a2",
   "metadata": {},
   "source": [
    "### 6.3.1 Conclusion on New Features\n",
    "1. The \"Total Acidity\" Stability\n",
    "- total_acidity feature shows a strong $1.00$ correlation with fixed acidity and $0.69$ with citric acid.\n",
    "    - The Strategy: This confirms that I can safely drop the three individual acid columns.\n",
    "\n",
    "* **Given the conclusion above**,\n",
    "    - **FINAL_COL_DATSET_BASE** = [total_acidity, residual sugar, total sulfur dioxide, chlorides, sulphates, alcohol, pH, density, quality_bin]\n",
    "    - **FINAL_COL_DATSET_OPT** = [total_acidity, total sulfur dioxide, chlorides, sulphates, alcohol, quality_bin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef181d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_COL_DATSET_BASE = ['total_acidity', 'residual sugar', 'total sulfur dioxide', 'chlorides', 'sulphates', 'alcohol', 'pH', 'density', 'quality_bin']\n",
    "FINAL_COL_DATSET_OPT = ['total_acidity', 'total sulfur dioxide', 'chlorides', 'sulphates', 'alcohol', 'quality_bin']\n",
    "\n",
    "df_WITH_outliers_baseCol = df_WITH_outliers_baseCol[FINAL_COL_DATSET_BASE]\n",
    "df_WITHOUT_outliers_baseCol = df_WITHOUT_outliers_baseCol[FINAL_COL_DATSET_BASE]\n",
    "df_WITH_outliers_optCol = df_WITH_outliers_optCol[FINAL_COL_DATSET_OPT]\n",
    "df_WITHOUT_outliers_optCol = df_WITHOUT_outliers_optCol[FINAL_COL_DATSET_OPT]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399807a",
   "metadata": {},
   "source": [
    "## 6.4 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fe672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the set into train and test\n",
    "\t\t\t\t\n",
    "predictors_base = ['total_acidity', 'residual sugar', 'total sulfur dioxide', 'chlorides', 'sulphates', 'alcohol', 'pH', 'density']\n",
    "predictors_opt = ['total_acidity', 'total sulfur dioxide', 'chlorides', 'sulphates', 'alcohol']\n",
    "target = 'quality_bin'\n",
    "\n",
    "X_WITH_outliers_baseCol = df_WITH_outliers_baseCol.drop(target, axis = 1)[predictors_base]\n",
    "X_WITHOUT_outliers_baseCol = df_WITHOUT_outliers_baseCol.drop(target, axis = 1)[predictors_base]\n",
    "X_WITH_outliers_optCol = df_WITH_outliers_optCol.drop(target, axis= 1)[predictors_opt]\n",
    "X_WITHOUT_outliers_optCol = df_WITHOUT_outliers_optCol.drop(target, axis= 1)[predictors_opt]\n",
    "y = df_WITH_outliers_baseCol[target]\n",
    "\n",
    "X_train_WITH_outliers_baseCol, X_test_WITH_outliers_baseCol, y_train, y_test = train_test_split(X_WITH_outliers_baseCol, y, test_size = 0.2, random_state = 10)\n",
    "X_train_WITHOUT_outliers_baseCol, X_test_WITHOUT_outliers_baseCol = train_test_split(X_WITHOUT_outliers_baseCol, test_size = 0.2, random_state = 10)\n",
    "X_train_WITH_outliers_optCol, X_test_WITH_outliers_optCol, y_train, y_test = train_test_split(X_WITH_outliers_optCol, y, test_size = 0.2, random_state = 10)\n",
    "X_train_WITHOUT_outliers_optCol, X_test_WITHOUT_outliers_optCol = train_test_split(X_WITHOUT_outliers_optCol, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and MIN_MAX Scaling\n",
    "# X_train_WITH_outliers_baseCol \n",
    "# X_train_WITHOUT_outliers_baseCol \n",
    "# X_train_WITH_outliers_optCol\n",
    "# X_train_WITHOUT_outliers_optCol\n",
    "\n",
    "# WE NEED TO SAVE 3 OCCURRENCE: 1) DATASET PLANE (WITH NO CHANGES), 2) DATASET NORMALIZED, 3) DATASET MIN-MAX\n",
    "\n",
    "## NORMALIZATION\n",
    "### WITH OUTLIERS\n",
    "# Base Columns\n",
    "norm_WITH_outliers_baseCol = StandardScaler() # StandardScaler(), used to \"normalize\" or \"resize\" your data so that all your features are on the same scale.\n",
    "# .fit(), The \"Learning\"\n",
    "norm_WITH_outliers_baseCol.fit(X_train_WITH_outliers_baseCol) # Phase. the scaler (or model) looks at your data and calculates the necessary parameters. It does not change the data; it only learns from it.\n",
    "# Opt. Columns\n",
    "norm_WITH_outliers_optCol = StandardScaler()\n",
    "norm_WITH_outliers_optCol.fit(X_train_WITH_outliers_optCol)\n",
    "\n",
    "# .transform(), The \"Applying\" Phase.\n",
    "# NOTE: .fit_transform() — The \"Shortcut\" --> This is simply a convenience method that does both steps at the same time on the same piece of data. --- Use this on your Training Data ---\n",
    "# Base Columns\n",
    "X_train_WITH_outliers_baseCol_norm = norm_WITH_outliers_baseCol.transform(X_train_WITH_outliers_baseCol) # This uses the parameters calculated during .fit() to actually modify the data.\n",
    "X_train_WITH_outliers_baseCol_norm = pd.DataFrame(X_train_WITH_outliers_baseCol_norm, index = X_train_WITH_outliers_baseCol.index, columns = predictors_base) # We need to convert to DataFrame the transform with this variable\n",
    "X_test_WITH_outliers_baseCol_norm = norm_WITH_outliers_baseCol.transform(X_test_WITH_outliers_baseCol)\n",
    "X_test_WITH_outliers_baseCol_norm = pd.DataFrame(X_test_WITH_outliers_baseCol_norm, index = X_test_WITH_outliers_baseCol.index, columns = predictors_base)\n",
    "# Opt. Columns\n",
    "X_train_WITH_outliers_optCol_norm = norm_WITH_outliers_optCol.transform(X_train_WITH_outliers_optCol)\n",
    "X_train_WITH_outliers_optCol_norm = pd.DataFrame(X_train_WITH_outliers_optCol_norm, index = X_train_WITH_outliers_optCol.index, columns = predictors_opt)\n",
    "X_test_WITH_outliers_optCol_norm = norm_WITH_outliers_optCol.transform(X_test_WITH_outliers_optCol)\n",
    "X_test_WITH_outliers_optCol_norm = pd.DataFrame(X_test_WITH_outliers_optCol_norm, index = X_test_WITH_outliers_optCol.index, columns = predictors_opt)\n",
    "\n",
    "### WITHOUT OUTLIERS\n",
    "# Base Columns\n",
    "norm_WITHOUT_outliers_baseCol = StandardScaler()\n",
    "norm_WITHOUT_outliers_baseCol.fit(X_train_WITHOUT_outliers_baseCol)\n",
    "# Opt. Columns\n",
    "norm_WITHOUT_outliers_optCol = StandardScaler()\n",
    "norm_WITHOUT_outliers_optCol.fit(X_train_WITHOUT_outliers_optCol)\n",
    "# Base Columns\n",
    "X_train_WITHOUT_outliers_baseCol_norm = norm_WITHOUT_outliers_baseCol.transform(X_train_WITHOUT_outliers_baseCol)\n",
    "X_train_WITHOUT_outliers_baseCol_norm = pd.DataFrame(X_train_WITHOUT_outliers_baseCol_norm, index = X_train_WITHOUT_outliers_baseCol.index, columns = predictors_base)\n",
    "X_test_WITHOUT_outliers_baseCol_norm = norm_WITHOUT_outliers_baseCol.transform(X_test_WITHOUT_outliers_baseCol)\n",
    "X_test_WITHOUT_outliers_baseCol_norm = pd.DataFrame(X_test_WITHOUT_outliers_baseCol_norm, index = X_test_WITHOUT_outliers_baseCol.index, columns = predictors_base)\n",
    "# Opt. Columns\n",
    "X_train_WITHOUT_outliers_optCol_norm = norm_WITHOUT_outliers_optCol.transform(X_train_WITHOUT_outliers_optCol)\n",
    "X_train_WITHOUT_outliers_optCol_norm = pd.DataFrame(X_train_WITHOUT_outliers_optCol_norm, index = X_train_WITHOUT_outliers_optCol.index, columns = predictors_opt)\n",
    "X_test_WITHOUT_outliers_optCol_norm = norm_WITHOUT_outliers_optCol.transform(X_test_WITHOUT_outliers_optCol)\n",
    "X_test_WITHOUT_outliers_optCol_norm = pd.DataFrame(X_test_WITHOUT_outliers_optCol_norm, index = X_test_WITHOUT_outliers_optCol.index, columns = predictors_opt)\n",
    "\n",
    "\n",
    "## SCALED MIN_MAX\n",
    "### WITH OUTLIERS\n",
    "# Base Columns\n",
    "scaler_WITH_outliers_baseCol = MinMaxScaler() # MinMaxScaler() is a scaling technique that transforms the data so that all values fall within a specific range, most commonly between 0 and 1.\n",
    "scaler_WITH_outliers_baseCol.fit(X_train_WITH_outliers_baseCol)\n",
    "# Opt. Columns\n",
    "scaler_WITH_outliers_optCol = MinMaxScaler() # MinMaxScaler() is a scaling technique that transforms the data so that all values fall within a specific range, most commonly between 0 and 1.\n",
    "scaler_WITH_outliers_optCol.fit(X_train_WITH_outliers_optCol)\n",
    "# Base Columns\n",
    "X_train_WITH_outliers_baseCol_scal = scaler_WITH_outliers_baseCol.transform(X_train_WITH_outliers_baseCol)\n",
    "X_train_WITH_outliers_baseCol_scal = pd.DataFrame(X_train_WITH_outliers_baseCol_scal, index = X_train_WITH_outliers_baseCol.index, columns = predictors_base)\n",
    "X_test_WITH_outliers_baseCol_scal = scaler_WITH_outliers_baseCol.transform(X_test_WITH_outliers_baseCol)\n",
    "X_test_WITH_outliers_baseCol_scal = pd.DataFrame(X_test_WITH_outliers_baseCol_scal, index = X_test_WITH_outliers_baseCol.index, columns = predictors_base)\n",
    "# Opt. Columns\n",
    "X_train_WITH_outliers_optCol_scal = scaler_WITH_outliers_optCol.transform(X_train_WITH_outliers_optCol)\n",
    "X_train_WITH_outliers_optCol_scal = pd.DataFrame(X_train_WITH_outliers_optCol_scal, index = X_train_WITH_outliers_optCol.index, columns = predictors_opt)\n",
    "X_test_WITH_outliers_optCol_scal = scaler_WITH_outliers_optCol.transform(X_test_WITH_outliers_optCol)\n",
    "X_test_WITH_outliers_optCol_scal = pd.DataFrame(X_test_WITH_outliers_optCol_scal, index = X_test_WITH_outliers_optCol.index, columns = predictors_opt)\n",
    "\n",
    "### WITHOUT OUTLIERS\n",
    "# Base Columns\n",
    "scaler_WITHOUT_outliers_baseCol = MinMaxScaler()\n",
    "scaler_WITHOUT_outliers_baseCol.fit(X_train_WITHOUT_outliers_baseCol)\n",
    "# Opt. Columns\n",
    "scaler_WITHOUT_outliers_optCol = MinMaxScaler()\n",
    "scaler_WITHOUT_outliers_optCol.fit(X_train_WITHOUT_outliers_optCol)\n",
    "# Base Columns\n",
    "X_train_WITHOUT_outliers_baseCol_scal = scaler_WITHOUT_outliers_baseCol.transform(X_train_WITHOUT_outliers_baseCol)\n",
    "X_train_WITHOUT_outliers_baseCol_scal = pd.DataFrame(X_train_WITHOUT_outliers_baseCol_scal, index = X_train_WITHOUT_outliers_baseCol.index, columns = predictors_base)\n",
    "X_test_WITHOUT_outliers_baseCol_scal = scaler_WITHOUT_outliers_baseCol.transform(X_test_WITHOUT_outliers_baseCol)\n",
    "X_test_WITHOUT_outliers_baseCol_scal = pd.DataFrame(X_test_WITHOUT_outliers_baseCol_scal, index = X_test_WITHOUT_outliers_baseCol.index, columns = predictors_base)\n",
    "# Opt. Columns\n",
    "X_train_WITHOUT_outliers_optCol_scal = scaler_WITHOUT_outliers_optCol.transform(X_train_WITHOUT_outliers_optCol)\n",
    "X_train_WITHOUT_outliers_optCol_scal = pd.DataFrame(X_train_WITHOUT_outliers_optCol_scal, index = X_train_WITHOUT_outliers_optCol.index, columns = predictors_opt)\n",
    "X_test_WITHOUT_outliers_optCol_scal = scaler_WITHOUT_outliers_optCol.transform(X_test_WITHOUT_outliers_optCol)\n",
    "X_test_WITHOUT_outliers_optCol_scal = pd.DataFrame(X_test_WITHOUT_outliers_optCol_scal, index = X_test_WITHOUT_outliers_optCol.index, columns = predictors_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d00b0",
   "metadata": {},
   "source": [
    "### 6.4.1 Testing Data Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Table with train data modified WITH outliers and baseCol.\")\n",
    "print(tabulate(X_train_WITH_outliers_baseCol_norm.head(), headers= \"keys\", tablefmt= \"psql\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e503db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Table with train data modified WITH outliers and optCol. Normalization of the data embedded.\")\n",
    "print(tabulate(X_train_WITHOUT_outliers_optCol_norm.head(), headers= \"keys\", tablefmt= \"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279ddaa",
   "metadata": {},
   "source": [
    "### 6.4.2 Saving all of the DATASET\n",
    "    train, test with all the variance we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7366b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETS that have been created so far in previous steps from the feature engineering\n",
    "# BaseCol\n",
    "X_train_WITH_outliers_baseCol.to_excel(\"../data/processed/X_train_WITH_outliers_baseCol.xlsx\", index = False)\n",
    "X_train_WITH_outliers_baseCol_norm.to_excel(\"../data/processed/X_train_WITH_outliers_baseCol_norm.xlsx\", index = False)\n",
    "X_train_WITH_outliers_baseCol_scal.to_excel(\"../data/processed/X_train_WITH_outliers_baseCol_scal.xlsx\", index = False)\n",
    "X_train_WITHOUT_outliers_baseCol.to_excel(\"../data/processed/X_train_WITHOUT_outliers_baseCol.xlsx\", index = False)\n",
    "X_train_WITHOUT_outliers_baseCol_norm.to_excel(\"../data/processed/X_train_WITHOUT_outliers_baseCol_norm.xlsx\", index = False)\n",
    "X_train_WITHOUT_outliers_baseCol_scal.to_excel(\"../data/processed/X_train_WITHOUT_outliers_baseCol_scal.xlsx\", index = False)\n",
    "\n",
    "X_test_WITH_outliers_baseCol.to_excel(\"../data/processed/X_test_WITH_outliers_baseCol.xlsx\", index = False)\n",
    "X_test_WITH_outliers_baseCol_norm.to_excel(\"../data/processed/X_test_WITH_outliers_baseCol_norm.xlsx\", index = False)\n",
    "X_test_WITH_outliers_baseCol_scal.to_excel(\"../data/processed/X_test_WITH_outliers_baseCol_scal.xlsx\", index = False)\n",
    "X_test_WITHOUT_outliers_baseCol.to_excel(\"../data/processed/X_test_WITHOUT_outliers_baseCol.xlsx\", index = False)\n",
    "X_test_WITHOUT_outliers_baseCol_norm.to_excel(\"../data/processed/X_test_WITHOUT_outliers_baseCol_norm.xlsx\", index = False)\n",
    "X_test_WITHOUT_outliers_baseCol_scal.to_excel(\"../data/processed/X_test_WITHOUT_outliers_baseCol_scal.xlsx\", index = False)\n",
    "\n",
    "# OptCol\n",
    "X_train_WITH_outliers_optCol.to_excel(\"../data/processed/X_train_WITH_outliers_optCol.xlsx\", index = False)\n",
    "X_train_WITH_outliers_optCol_norm.to_excel(\"../data/processed/X_train_WITH_outliers_optCol_norm.xlsx\", index = False)\n",
    "X_train_WITH_outliers_optCol_scal.to_excel(\"../data/processed/X_train_WITH_outliers_optCol_scal.xlsx\", index = False)\n",
    "X_train_WITHOUT_outliers_optCol.to_excel(\"../data/processed/X_train_WITHOUT_outliers_optCol.xlsx\", index = False)\n",
    "X_train_WITHOUT_outliers_optCol_norm.to_excel(\"../data/processed/X_train_WITHOUT_outliers_optCol_norm.xlsx\", index = False)\n",
    "X_train_WITHOUT_outliers_optCol_scal.to_excel(\"../data/processed/X_train_WITHOUT_outliers_optCol_scal.xlsx\", index = False)\n",
    "\n",
    "X_test_WITH_outliers_optCol.to_excel(\"../data/processed/X_test_WITH_outliers_optCol.xlsx\", index = False)\n",
    "X_test_WITH_outliers_optCol_norm.to_excel(\"../data/processed/X_test_WITH_outliers_optCol_norm.xlsx\", index = False)\n",
    "X_test_WITH_outliers_optCol_scal.to_excel(\"../data/processed/X_test_WITH_outliers_optCol_scal.xlsx\", index = False)\n",
    "X_test_WITHOUT_outliers_optCol.to_excel(\"../data/processed/X_test_WITHOUT_outliers_optCol.xlsx\", index = False)\n",
    "X_test_WITHOUT_outliers_optCol_norm.to_excel(\"../data/processed/X_test_WITHOUT_outliers_optCol_norm.xlsx\", index = False)\n",
    "X_test_WITHOUT_outliers_optCol_scal.to_excel(\"../data/processed/X_test_WITHOUT_outliers_optCol_scal.xlsx\", index = False)\n",
    "\n",
    "y_train.to_excel(\"../data/processed/y_train.xlsx\", index = False)\n",
    "y_test.to_excel(\"../data/processed/y_test.xlsx\", index = False)\n",
    "\n",
    "# SCALERS --> Saving the models\n",
    "\n",
    "with open(\"../models/norm_WITH_outliers_baseCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(norm_WITH_outliers_baseCol, file)\n",
    "with open(\"../models/norm_WITH_outliers_optCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(norm_WITH_outliers_optCol, file)\n",
    "with open(\"../models/norm_WITHOUT_outliers_baseCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(norm_WITHOUT_outliers_baseCol, file)\n",
    "with open(\"../models/norm_WITHOUT_outliers_optCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(norm_WITHOUT_outliers_optCol, file)\n",
    "with open(\"../models/scaler_WITH_outliers_baseCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(scaler_WITH_outliers_baseCol, file)\n",
    "with open(\"../models/scaler_WITH_outliers_optCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(scaler_WITH_outliers_optCol, file)\n",
    "with open(\"../models/scaler_WITHOUT_outliers_baseCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(scaler_WITHOUT_outliers_baseCol, file)\n",
    "with open(\"../models/scaler_WITHOUT_outliers_optCol.pkl\", \"wb\") as file:\n",
    "  pickle.dump(scaler_WITHOUT_outliers_optCol, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5b642",
   "metadata": {},
   "source": [
    "# Step 7. Feature Selection\n",
    "    - Test all dataset \"roughly\" and I will keep the best score. Keeping all variable.\n",
    "    - The model need to be trained entirely.\n",
    "    - If results meet desire objective --> YEAH! WE?VE FINISHED.\n",
    "    - If not, move back to step 6 and repeat the process from step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f50f2f",
   "metadata": {},
   "source": [
    "* **IMPORTANT MESSAGE**: \n",
    "- Although I have runned the feature selection to check best freatures or variable using SelectKBest, I will run my KNN model with the base al optimized columns define before to compare Accuracy\n",
    "- For a better training an practicing and checking if the Accuracy of the KNN model cqan be improved, I might go back to this point of feature selection and try running the model with SelectKBest.\n",
    "- Thank you all for your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d42ea",
   "metadata": {},
   "source": [
    "# Step 8. Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_base = [\n",
    "    X_train_WITH_outliers_baseCol,\n",
    "    X_train_WITH_outliers_baseCol_norm,\n",
    "    X_train_WITH_outliers_baseCol_scal,\n",
    "    X_train_WITHOUT_outliers_baseCol,\n",
    "    X_train_WITHOUT_outliers_baseCol_norm,\n",
    "    X_train_WITHOUT_outliers_baseCol_scal,\n",
    "    ]\n",
    "\n",
    "datasets_opt = [\n",
    "    X_train_WITH_outliers_optCol,\n",
    "    X_train_WITH_outliers_optCol_norm,\n",
    "    X_train_WITH_outliers_optCol_scal,\n",
    "    X_train_WITHOUT_outliers_optCol,\n",
    "    X_train_WITHOUT_outliers_optCol_norm,\n",
    "    X_train_WITHOUT_outliers_optCol_scal,\n",
    "    ]\n",
    "\n",
    "models_base = []\n",
    "metrics_base = []\n",
    "for dataset in datasets_base:\n",
    "  model = KNeighborsClassifier() # KNN model\n",
    "  model.fit(dataset, y_train) # To train the model\n",
    "  y_pred = model.predict(dataset)\n",
    "  metric = accuracy_score(y_train, y_pred)\n",
    "  metrics_base.append(metric)\n",
    "  models_base.append(model)\n",
    "\n",
    "best_metric_base = max(metrics_base)\n",
    "best_index_base = metrics_base.index(best_metric_base)\n",
    "print(f\"This is the list of accu. score: {metrics_base}\\nThe best metric: {best_metric_base}\\nThe Best index: {best_index_base}\")\n",
    "print(\"The best DataSet is: \", datasets_base[best_index_base])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_opt = []\n",
    "metrics_opt = []\n",
    "for dataset in datasets_opt:\n",
    "  model = KNeighborsClassifier()\n",
    "  model.fit(dataset, y_train) # To train the model\n",
    "  y_pred = model.predict(dataset)\n",
    "  metric = accuracy_score(y_train, y_pred)\n",
    "  metrics_opt.append(metric)\n",
    "  models_opt.append(model)\n",
    "\n",
    "best_metric_opt = max(metrics_opt)\n",
    "best_index_opt = metrics_opt.index(best_metric_opt)\n",
    "print(f\"This is the list of accu. score: {metrics_opt}\\nThe best metric: {best_metric_opt}\\nThe Best index: {best_index_opt}\")\n",
    "print(\"The best DataSet is: \", datasets_opt[best_index_opt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d37b0",
   "metadata": {},
   "source": [
    "## 8.1 Evaluate Performance on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using the 'Champion' data (Index 4 for base from the list)\n",
    "# Let's assume you're using the Normalized (StandardScaler) version\n",
    "knn_base = KNeighborsClassifier()\n",
    "knn_base.fit(X_train_WITHOUT_outliers_baseCol_norm, y_train)\n",
    "\n",
    "# 2. Predict on the unseen Test Set\n",
    "y_pred_base = knn_base.predict(X_test_WITHOUT_outliers_baseCol_norm)\n",
    "\n",
    "# 3. Create the Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_base)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= ['Low Quality', 'High Quality'])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap= 'Blues')\n",
    "plt.title('Model with base Columns selection: Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 4. Detailed Report\n",
    "print(classification_report(y_test, y_pred_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342edc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using the 'Champion' data (Index 2 for optimized from the list)\n",
    "knn_opt = KNeighborsClassifier()\n",
    "knn_opt.fit(X_train_WITH_outliers_optCol_scal, y_train)\n",
    "\n",
    "# 2. Predict on the unseen Test Set\n",
    "y_pred_opt = knn_opt.predict(X_test_WITH_outliers_optCol_scal)\n",
    "\n",
    "# 3. Create the Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= ['Low Quality', 'High Quality'])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap= 'Blues')\n",
    "plt.title('Model with optimized Columns selection: Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 4. Detailed Report\n",
    "print(classification_report(y_test, y_pred_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c1ffe",
   "metadata": {},
   "source": [
    "# Step 9. Optimize k.\n",
    "- Create a loop to test different k values (e.g., from 1 to 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf391c71",
   "metadata": {},
   "source": [
    "## 9.1 Plot Accuracy vs k (Find the best value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa54e81",
   "metadata": {},
   "source": [
    "### 9.1.1 Conclusion on K Value Chart\n",
    "- To optimize the model, I tested $k$ values from 1 to 20.\n",
    "- The K Value range I have adjusted the step to \"2\" avoiding having tight voting when the number is even.\n",
    "- The results indicate that $k=17$ is the optimal hyperparameter. \n",
    "- Beyond this point, accuracy declines as the model begins to underfit, incorporating irrelevant neighbors into the classification decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff89c44",
   "metadata": {},
   "source": [
    "## 9.2 Save the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0684b",
   "metadata": {},
   "source": [
    "# Step 10. Feeling Confident (test the model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
